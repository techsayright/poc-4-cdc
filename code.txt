docker-compose up -d

## Loading data into Postgres

docker run -it --rm --network=poc-4-cdc_default \
         -v "$PWD":/home/config/Data \
         debezium/postgres:11 psql -h postgres -U postgres

Password = postgres

At the command line:

```
CREATE DATABASE students;
\connect students;
```

Load our admission data table:

```
CREATE TABLE admission
(student_id INTEGER, gre INTEGER, toefl INTEGER, cpga DOUBLE PRECISION, admit_chance DOUBLE PRECISION,
CONSTRAINT student_id_pk PRIMARY KEY (student_id));

\copy admission FROM '/home/config/Data/admit_1.csv' DELIMITER ',' CSV HEADER;
```

insert into admission values ( 12345, 98, 98, 0.09,0.87);
update admission set gre = 99 where student_id = 12345;
delete from admission where student_id = 12345;


Load the research data table with:

```
CREATE TABLE research
(student_id INTEGER, rating INTEGER, research INTEGER,
PRIMARY KEY (student_id));

\copy research FROM '/home/config/Data/research_1.csv' DELIMITER ',' CSV HEADER;
```


##if you want to create topic manually

docker-compose exec broker kafka-topics --create \
    --bootstrap-server localhost:9092 \
    --partitions 1 \
    --replication-factor 1 \
    --topic psg-admission


## Connect Postgres database as a source to Kafka
```
curl -X POST -H "Accept:application/json" -H "Content-Type: application/json" \
      --data @postgres-source.json http://localhost:8083/connectors
```


for showing connectors and its status
```
curl -H "Accept:application/json" localhost:8083/connectors/

curl localhost:8083/connectors/s3-connector/status
```
##for showing logs of connections if err exist or not
docker logs -f connect


The two tables in the `students` database will now show up as topics in Kafka.
You can check this by entering the Kafka container:

```
docker exec -it <kafka-container-id> /bin/bash
```

and listing the available topics:

```
/usr/bin/kafka-topics --list --zookeeper zookeeper:2181
```

```
/usr/bin/kafka-console-consumer --bootstrap-server localhost:9092 --topic test --from-beginning

```


## Create tables in KSQL

Bring up a KSQL server command line client as a container:

```
docker-compose exec ksqldb-cli ksql http://ksqldb-server:8088 

                          or

docker run --network postgres-kafka-demo_default \
           --interactive --tty --rm \
           confluentinc/cp-ksql-cli:latest \
           http://ksql-server:8088
```

To see your updates, a few settings need to be configured by first running:

```
set 'commit.interval.ms'='2000';
set 'cache.max.bytes.buffering'='10000000';
set 'auto.offset.reset'='earliest';
```

### Mirror Postgres tables

The Postgres table topics will be visible in KSQL, and we will create
KSQL streams to auto update KSQL tables mirroring the Postgres tables:

```
SHOW TOPICS;

print 'topic' from beginning;

CREATE STREAM admission_src (student_id INTEGER, gre INTEGER, toefl INTEGER, cpga DOUBLE, admit_chance DOUBLE)\
WITH (KAFKA_TOPIC='dbserver1.public.admission', VALUE_FORMAT='AVRO');

select * from admission_src emit changes;


##for table joining reason we have to partition data by student_id

CREATE STREAM admission_src_rekey WITH (PARTITIONS=1) AS \
SELECT * FROM admission_src PARTITION BY student_id;

SHOW STREAMS;

##it will also create new table nd topic named ADMISSION_SRC_REKEY

CREATE TABLE admission (student_id INTEGER PRIMARY KEY, gre INTEGER, toefl INTEGER, cpga DOUBLE, admit_chance DOUBLE)\
WITH (KAFKA_TOPIC='ADMISSION_SRC_REKEY', VALUE_FORMAT='AVRO');

SHOW TABLES;

CREATE STREAM research_src (student_id INTEGER, rating INTEGER, research INTEGER)\
WITH (KAFKA_TOPIC='local.students.research', VALUE_FORMAT='AVRO');

CREATE STREAM research_src_rekey WITH (PARTITIONS=1) AS \
SELECT * FROM research_src PARTITION BY student_id;

CREATE TABLE research (student_id INTEGER PRIMARY KEY, rating INTEGER, research INTEGER)\
WITH (KAFKA_TOPIC='RESEARCH_SRC_REKEY', VALUE_FORMAT='AVRO');
```

Currently KSQL uses uppercase casing convention for stream, table, and field
names.

### Create downstream tables

We will create a new KSQL streaming table to join students' chance of
admission with research experience.

```
CREATE TABLE research_boost AS \
  SELECT a.student_id as student_id, \
         a.admit_chance as admit_chance, \
         r.research as research \
  FROM admission a \
  LEFT JOIN research r on a.student_id = r.student_id;
```

and another table calculating the average chance of admission for
students with and without research experience:

```
CREATE TABLE research_ave_boost WITH (KAFKA_TOPIC='research_ave_boost', VALUE_FORMAT='AVRO') AS SELECT research, SUM(admit_chance)/COUNT(admit_chance) as ave_chance FROM research_boost GROUP BY research;


```

## Add a connector to sink a KSQL table to s3
note: s3 bucket should be created before run below command
```
curl -X POST -H "Accept:application/json" -H "Content-Type: application/json" \
--data @custom-connectors/connectors/s3-sink.json http://localhost:8083/connectors
```

## Update the source Postgres tables and watch the s3 bucket update



##for MongoDB------------

docker-compose exec mongo1 /usr/bin/mongo --eval 'rs.initiate({_id : "rs0",members:[{ _id : 0, host : "mongo1:27017", priority: 1.0 },{ _id : 1, host : "mongo2:27017", priority: 0.5 }]})'

docker exec -it mongo1 bash

#to start mongo
mongo

use mydb
db.colors.insert([
{ _id : 1005, first_name : 'Bob', last_name : 'Hopper', email : 'thebob@example.com' }
]);

db.colors.find().pretty()
